{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0cd682-e5e9-4a48-9521-f57f63b7e687",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Trasnfer of Learning Assignment\n",
    "This assignment you should complete this notebook. This is the transfer of learning assignment. You should transfer learning from ResNet50 and predict from MNIST dataset. Codes for loading and assigning train and test sets are already provided. You need to use your university ID for random seed. Also, classification should be developed with decision tress\n",
    "\n",
    "<div style=\"display: inline-block\">\n",
    "  <img src=\"https://thedatafrog.com/static/blog/images/2019/10/dessin_transfer_learning_crop-1-1024x794.jpg\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "<div style=\"display: inline-block\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/0*tH9evuOFqk8F41FG.png\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "Image Source: thedatafrog.com & towardsdatascience.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af9086d-8e69-4157-a029-2cd63f6c9537",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "part 1: Install the required packages!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0a5e54f-4968-4a72-bd93-3c56e159cb39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following function is a command used to restart the Python interpreter running on the Spark driver node. The purpose of this command is to reload any libraries or modules that have been updated since the interpreter was last started, so that the updated code can be used in subsequent PySpark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c565dfc-7637-40d4-b376-8f1547c914d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b824ab9-b56a-43b7-92b4-2141de6c3eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's time to import packages and starting a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379ed201-c112-450d-8cc8-51044d306238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2: you need to figure out which libraries you should import\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4fee4a-c869-4571-8662-5264796cd22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we use the subsets of the dataset to use a smaller portion of the images for training and testing. This is only for making it excutable by low power PCs and/or community version of Databricks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb46f7d-4985-49ad-88c0-5f2b9300bff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load mnist dataset from TensorFlow\n",
    "(train_images, train_labels), (test_images, test_labels)  = datasets.mnist.load_data()\n",
    "# Subset the dataset\n",
    "# part 3: use your numerical part of your ID as the random seed and then randomly pick 1000 observations of train set and 100 observations of the test set\n",
    "# then instead of training on whole dataset just do training and evaluation on the subsets\n",
    "random.seed(....) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a81f4bc-0de6-44d2-90aa-1c53f3a28708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This creates Spark DataFrames from the NumPy arrays by flattening the image arrays, converting them to Vectors, and creating tuples with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2407466a-d3d8-4d2f-9a08-004d46f2234f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark DataFrames from NumPy arrays\n",
    "train_df = spark.createDataFrame([(Vectors.dense(img.flatten().tolist()), int(label)) for img, label in zip(train_images, train_labels)], [\"features\", \"label\"])\n",
    "test_df = spark.createDataFrame([(Vectors.dense(img.flatten().tolist()), int(label)) for img, label in zip(test_images, test_labels)], [\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7528f138-ac0f-4480-a133-93a216991561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from images using pre-trained model\n",
    "model = .... # part 4: complete this part with ResNet50 model\n",
    "\n",
    "def extract_features(img):\n",
    "    # Resize image to (224, 224, 3)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    # Preprocess image for ResNet50 model\n",
    "    img = preprocess_input(img)\n",
    "    # Extract features from image using ResNet50 model\n",
    "    features = model.predict(img.reshape(1, 224, 224, 3)).ravel()\n",
    "    # Return features as DenseVector\n",
    "    return DenseVector(features)\n",
    "\n",
    "# Convert NumPy arrays to list of images\n",
    "train_images_list = [train_images[i] for i in range(train_images.shape[0])]\n",
    "test_images_list = [test_images[i] for i in range(test_images.shape[0])]\n",
    "\n",
    "# Extract features from images and convert to Spark DataFrames\n",
    "train_df = spark.createDataFrame([(extract_features(img), int(label)) for img, label in zip(train_images_list, train_labels)], [\"features\", \"label\"])\n",
    "test_df = spark.createDataFrame([(extract_features(img), int(label)) for img, label in zip(test_images_list, test_labels)], [\"features\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0a303d4-4266-471d-b231-e25363a53580",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5: Train decision trees classification model on the extracted features\n",
    "# then you need to report accuracy of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739512ab-d595-4142-88e9-d1fa7d137498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "TOL_assignment",
   "notebookOrigID": 916878723204908,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
